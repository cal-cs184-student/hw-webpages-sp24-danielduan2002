<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Daniel Duan</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">TODO</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/banan.png" width="480px" />
          <figcaption align="middle">my banan is the bananiest banan</figcaption>
      </tr>
  </table>
</div>

<div>

<h2 align="middle">Overview</h2>
<p>
    In this project, I explored the core pathtracing algorithms that form the foundation 
    of physically-based renderers. This writeup will cover the high level math concepts 
    that form the basis of the algorithms, as well as the computer science data structures 
    that allow for effective abstraction and efficient computation. 
    <br><br>
    More than anything, this project heavily tested my patience. The bugs were awful, and waiting 
    and waiting for scenes to render was nerve-wracking, but the final products made it all worth it. 
    I learned a lot, and now I get to stare at cool looking pictures! (that I made!)
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<p>
  Ray generation in the rendering pipeline involved a sequence of transformation steps. 
  To render a point (x,y), it first gets translated to take it from image space to camera space, then 
  converted to world space using a provided camera-to-world matrix. 
  <br><br>
  After producing rays originating from the camera and pointing towards our world, I then took random samples for each pixel in the image. Averaging the radiance of the sampled 
  rays gave me the radiance that served as the pixel's color. 
  <br><br>
  Next, rendering primitives like triangles and circles required tests for 
  ray-primitive intersections. To check if a ray intersects a triangle in a 3D scene, 
  I used the ray equation and plane equation to compute the geometric parameter t. 
  The parameter t is then used to find the point of intersection, and then using 
  logic from Project 1 (Rasterizer), I determined whether the intersection point lies within the 
  triangle and is valid along the ray's direction. If valid, the ray's 
  maximum distance parameter is updated, and intersection details such as the distance, 
  normal, and material properties are tracked. To speed up this process, I used the Moller-Trombore 
  algorithm from lecture, which reduced the number of operations needed to compute t. 
  <br><br>
  <div align="middle">
    <table style="width:100%">
      <tr align="center">
        <td>
          <img src="images/rayplane.png" align="middle" width="300"/>
        </td>
        <td>
          <img src="images/raysphere.png" align="middle" width="300"/>
        </td>
      </tr>
    </table>
  </div>
  <br>
  Computing ray-sphere intersections was a similar process, using the ray equation and sphere 
  equation along with a parameter t to solve for the coordinate where the two are equal. 
  While a plane and ray will always intersect at exactly one point, ray-sphere intersections 
  broke down into more complex cases:
  <br>
  <br>1) A negative determinant indicates no intersection, return false
  <br>2) A determinant of 0 indicates 1 solution for t, set the ray's max_t and return t
  <br>3) A positive determinant indicates 2 real t solutions, update the ray's max_t to be the closer intersection
  <br><br>
</p>

<h3>
  Ray generation and primitive intersection allows us to perform normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBempty.png" align="middle" width="400px"/>
        <figcaption>A room built from triangles</figcaption>
      </td>
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>Two spheres have an important business meeting in our new room</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  While basic normal shading gets the job done, when given more complicated geometries,
  rendering takes a long time. This section explroes how a Bounding Volume Hierarchy (BVH) 
  allows us to speed up the computational complexity of ray-primitive intersection from O(n) to O(logn),
  a considerable speedup. The high-level idea behind BVH is that if a ray doesn't intersect the bounding box 
  of a set of primitives, then that ray doesn't intersect any of the primitives themselves, thus 
  we can do fewer ray-primitive tests.
  <br><br>
  <div align="middle">
    <img src="images/bvhslide.png" align="middle" width="200px"/>
  </div>
  <br><br>
  To construct the BVH, I started with a node whose bounding box contained all the primitives. 
  I then iterated through all these primitives, calculated their average centroid, and identified the 
  largest axis of the bounding box. Next, the bounding box is split in half along that axis (heuristic), 
  separating the primitives into two child sets, each enclosed in a smaller bounding box. This BVH construction process continues recursively for the left 
  and right children until the stopping condition (num primitives less than max_leaf_size) is met. 
  <br><br>
  The binary hierarchical groupings produced by the BVH algorithm are illustrated below.
  <br><br>
  <div align="middle">
    <table style="width:100%">
      <tr align="center">
        <td>
          <img src="images/bvh1.png" align="middle" width="250px"/>
        </td>
        <td>
          <img src="images/bvh2.png" align="middle" width="250px"/>
        </td>
        <td>
          <img src="images/bvh3.png" align="middle" width="250px"/>
        </td>
        <td>
          <img src="images/bvh4.png" align="middle" width="250px"/>
        </td>
      </tr>
    </table>
  </div>

</p>

<h3>
  Shown below are images with normal shading that contain a large number of primitives, along with render-time comparisons 
  with and without BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/maxplanck.png" align="middle" width="250"/>
        <figcaption>Max Planck</figcaption>
      </td>
      <td>
        <img src="images/plancklong.png" align="middle" width="300"/>
        <figcaption>Without BVH Acceleration: 934 seconds</figcaption>
      </td>
      <td>
        <img src="images/planckshort.png" align="middle" width="300"/>
        <figcaption>With BVH Acceleration: 0.16 seconds</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/cblucy.png" align="middle" width="250"/>
        <figcaption>Cblucy</figcaption>
      </td>
      <td>
        <img src="images/cblucylong.png" align="middle" width="300"/>
        <figcaption>Without BVH Acceleration: 2448 seconds</figcaption>
      </td>
      <td>
        <img src="images/cblucyshort.png" align="middle" width="300"/>
        <figcaption>With BVH Acceleration: 0.16 seconds</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  BVH Acceleration yields a several-thousand-fold speedup for complex geometries! Wow!
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<p>
  Direct lighting utilizes zero bounce lighting and one bounce lighting. Zero bounce lighting 
  is the lighting from the light itself, and one bounce lighting describes how 
  light bounces off a surface and illuminates nearby surfaces. 
  <br><br>
  The first approach to direct lighting is uniform hemisphere lighting. 
  This approach involves a projected ray hitting a point and casting rays in a 
  uniformly distributed manner across the hemisphere. This type of lighting provides
  an unbiased estimate of the lighting environment, and considers all directions equally.
  <br><br>
  The second approach, light importance sampling, focuses the computational effort on directions 
  which contribute more to the final image. These directions are usually determined by the proximity and intensity  
  of light sources. Light importance sampling assigns greater priority to rays targetting areas of more illumination,
  and thus reduces variance and accelerates convergence. This helps speed up render quality in 
  scenes with many light sources or complex lighting. 
</p>

<h3>
  Some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBbunny_H_32_16.png" align="middle" width="400px"/>
        <figcaption>CBbunny_H_32_16</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_32_16.png" align="middle" width="400px"/>
        <figcaption>CBbunny_32_16</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/CBspheres_H_32_16.png" align="middle" width="400px"/>
        <figcaption>CBspheres_H_32_16</figcaption>
      </td>
      <td>
        <img src="images/CBspheres_32_16.png" align="middle" width="400px"/>
        <figcaption>CBspheres_32_16</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Observations
</h3>
<p> 
  Uniform hemisphere sampling resultes in an image with more grainy lighting, whereas light sampling 
  produces images with smoother lighting and gradients on surfaces. Images using light sampling also appear 
  brighter than images rendered with uniform hemisphere sampling.
</p>

<h3>
  Single area light (Lighting Sampling), noise level comparisons in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBbunny_32_16_l1.png" align="middle" width="250px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_32_16_l4.png" align="middle" width="250px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBbunny_32_16_l16.png" align="middle" width="250px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_32_16_l64.png" align="middle" width="250px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Observations
</h3>
<p> 
  When rendering with fewer light rays, artifacts like large black dots appear and lead to messier shadows with less defined shape (1 light ray). 
  When more light rays are used, shadows get consolidated in shape, shadow edges and gradients get smoothed out (64 light ray).
</p>
<br>


<h2 align="middle">Part 4: Global Illumination</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<p>
  Building off of direct lighting, global illumination (or indirect lighting) introduces the idea that light rays can bounce off surfaces 
  recursively. My implementation of global illumination uses this recursive algorithm: 
  <br><br>1. If the ray reaches a depth of 0, it is considered finished and returns the radiance
  <br>2. Otherwise, the ray's one bounce radiance is computed. We do russian roulette aka some probability that we terminate. If we don't terminate, we sample a random incoming direction w_i based on w_o, and  
    recurse, scaling the resulting recursive value by the reflectance of the intersected material, lambert's cosine law, as well as inverses of the probabilities of getting that sample and russian roulette probability.
</p>
<h3>
  Some images rendered with global (direct and indirect) illumination, using 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/banan_1024.png" align="middle" width="400px"/>
        <figcaption>High quality banan</figcaption>
      </td>
      <td>
        <img src="images/spheres_1024.png" align="middle" width="400px"/>
        <figcaption>High quality sphere</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendered views of a scene with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. 
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/onlydirect.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination</figcaption>
      </td>
      <td>
        <img src="images/onlyindirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  With only direct illumination, sections of an object that face the light source are very bright, and 
  sections of an object that face away from the light source and are obstructed appear very dark. 

  With only indirect illumination, overall lighting has a much lower contrast, since light is scattered 
  more evenly throughout the scene compared to direct illumination. In the case of the spheres, the bottoms of 
  the spheres appear slightly brighter than their tops, which is a result of light from the ceiling light source 
  bouncing off of the ground and hitting the spheres' undersides.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/depth1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/depth2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/depth3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/depth100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  At higher depths, light rays bounce more, and thus we get brighter images. Shadows are still 
  shadows, but are no longer as dark. With depth 1, the ceiling was pitch black, but at depth 100, the 
  the ceiling is only a bit darker than the floor.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/sample1.png" align="middle" width="200"/>
        <figcaption>1 sample per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/sample2.png" align="middle" width="200"/>
        <figcaption>2 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/sample4.png" align="middle" width="200"/>
        <figcaption>4 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/sample8.png" align="middle" width="200"/>
        <figcaption>8 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/sample16.png" align="middle" width="200"/>
        <figcaption>16 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/sample64.png" align="middle" width="200"/>
        <figcaption>64 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/sample1024.png" align="middle" width="200"/>
        <figcaption>1024 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  With very few samples per pixel, the rendered image is very grainy and noisy due to the fact that only 
  4 light rays are used. At higher samples per pixel, that noise gets averaged out, and the resulting 
  image has much smoother lighting.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<p>
  As the previous sections have shown, Monte Carlo path tracing is an effective way to generate realistic images. However, it also tends to produce a good amount of noise.
  While noise can be reduced simply by increasing the number of samples per pixel, not all pixels require the same amount of samples to converge.
  Adaptive sampling tackles this issue by focusing the samples into the more noisy parts of the image. It does this by using statistics to detect if a pixel has converged as ray samples are traced through it. 
  For my implementation of adaptive sampling, I added logic inside the sampling loop of raytrace_pixel to compute the mean, variance, and standard deviation of the samples so far. I then used this equation to measure the pixel’s convergence, and if it converged, break out of the loop to stop taking more pixel samples.

</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunbun.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunbunonhallucinogens.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheredudes.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheredudesonhallucinogens.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  The red ares indicate that the pixels converged more slowly, suggesting that indirect lighting induced 
  more significant overall lighting changes. Shadows and corners require more samples to converge.
  In blue areas, the sample converges more quickly. These areas tend to be regions that are 
  directly exposed to the light source, which means its radiance / lighting 
  can be determined quickly. 
</p>


</body>
</html>
